{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6b56d7-3ac1-419d-8d6c-8dbca7e5dc1b",
   "metadata": {},
   "source": [
    "## TOKEN CALCULATOR with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9415d2-31f9-4824-8c3c-a1c4bb2a1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'cl100k_base'>\n",
      "<Encoding 'cl100k_base'>\n",
      "[83, 1609, 5963, 374, 2294]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(encoding)\n",
    "#Encoding by model name\n",
    "encoding1 = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "print(encoding1)\n",
    "#turn text into tokens\n",
    "print(encoding.encode(\"tiktoken is great\"))\n",
    "len(encoding.encode(\"tiktoken is great\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae72ef7-dada-4356-9c86-4703611107dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1609, 5963, 374, 2294]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn text into tokens(ENCODE)\n",
    "Result = encoding.encode(\"tiktoken is great\")\n",
    "print(Result)\n",
    "len(encoding.encode(\"tiktoken is great\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3ddcb9-f835-49b8-aef0-59bb447b67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great+'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn tokens into text(DECODE)\n",
    "encoding.decode([83, 1609, 5963, 374, 2294, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a82b13-fdb0-4baf-9814-1f6a58bdd279",
   "metadata": {},
   "source": [
    "Refer: [this](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1e7c2a-d817-4dfb-a2f9-19e4f0c039c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b't', b'ik', b'token', b' is', b' great']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying decode on single individual token can be lossy so we use .decode_single_token_bytes()\n",
    "[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c3699-e6a7-4046-a6b2-e911f3f0c6be",
   "metadata": {},
   "source": [
    "## Finding token length for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5ab344-6138-4885-9561-8a8da9ee0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2028, 574, 264, 1633, 17879, 17219, 311, 445, 11237, 82, 323, 19089, 1690, 4860, 358, 1047, 13, 9930, 499, 13]\n",
      "token_length is 20\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "Encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "\n",
    "# Extract a Message Item from Message List\n",
    "tobe_tokenized_Object= [{\"role\": \"system\", \"content\": \"***************\"}, {\"role\": \"user\", \"content\": \"This was a very thorough introduction to LLMs and answered many questions I had. Thank you.\"}, {\"role\": \"user\", \"content\": \"Great to hear, glad it was helpful :) -MBGPT\"}]\n",
    "#THis encode method expects string so we cant pass message object for now, so lets only tokenize content's value for now\n",
    "tobe_tokenized_Text= tobe_tokenized_Object[1].get(\"content\")\n",
    "\n",
    "token_integers = Encoding.encode(tobe_tokenized_Text)\n",
    "print(token_integers)\n",
    "token_length = len(token_integers)\n",
    "print(f\"token_length is {token_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b97cd-307e-4420-838e-8811ad1b763b",
   "metadata": {},
   "source": [
    "**Number of tokens differ from model to model, because how the input are tokenized also differs. ChatCompletion Model like ChatGPT has different way of calcualting token considering the message format they use , [refer](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken#6-counting-tokens-for-chat-completions-api-calls) \n",
    "so total token = (token for pre-defined message format) + (encoded_value) + (repeated name which is -1)... \n",
    "So we have to implement custom token calculation function for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ff58e-a077-4740-b7ae-32a9dc8143ef",
   "metadata": {},
   "source": [
    "**Below function is meant to be use for Chat completion models like gpt-3.5-turbo and gpt-4 ( both use the token in the same way) encoded with\n",
    "cl100k_base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a95203-f248-493c-a698-0b00c8b65365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will check gpt-3.5-turbo-0125 and gpt-4-turbo\n",
    "\n",
    "def num_tokens_from_messages(messages,model):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        #this includes token for all keys inside a message\n",
    "        num_tokens+=tokens_per_message\n",
    "        #Iterate over all values inside a message\n",
    "        for key, value in message.items():\n",
    "            num_tokens+= len(encoding.encode(value))\n",
    "            #adding name\n",
    "            if key == \"name\":\n",
    "                num_tokens+=tokens_per_name\n",
    "        num_tokens+=3 #imagine it as a greeeting to model augmeneted at the start of each message\n",
    "        \n",
    "    return num_tokens\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "                    \n",
    "                             \n",
    "    \n",
    "    \n",
    "                             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
