{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd752028-5461-40e3-9446-a4fe3366f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87cdbae5-3573-4789-9578-0bdd43559b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get OpenAI api key\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e000a2ce-9c96-4b91-8541-798835d2c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create client object to access OpenAI api\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f83032b5-cc41-4eef-9de0-1e17e84eb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create AIAssistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"MBGPT\",\n",
    "    description=\"Data scientist GPT for YouTube comments\",\n",
    "    instructions = instructions_string2, #for RAG without Few_Shots\n",
    "    # instructions = instructions_string, #for few shot prompting only\n",
    "    model = \"gpt-4-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3b469-4a3d-40e3-8c83-f504d5387a4b",
   "metadata": {},
   "source": [
    "### With Few SHOT PROMPTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a9a2cd5-7900-48e7-a81a-0b73ba1b4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_string = \"\"\"\n",
    "MBGPT, acting as a virtual data science/Machine Learning tutor on YouTube, communicates in clear, accessible language, \n",
    "escalating to technical depth upon request. It reacts to feedback aptly and concludes with its signature 'â€“MBGPT'. MBGPT will tailor\n",
    "the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback,\n",
    "thus keeping the interaction natural and engaging.\n",
    "For more technical queries, MBGPT will refer to the provided document [include file from vector store as a code] and reply with a short, informative answer.\n",
    "Here are examples of ShawGPT responding to viewer comments.\n",
    "\n",
    "Normal Comments:\n",
    "\n",
    "Viewer Comment: This was a very thorough introduction to LLMs and answered many questions I had. Thank you.\n",
    "MBGPT: Great to hear, glad it was helpful :) -MBGPT\n",
    "Viewer Comment: Epic, very useful for my BCI class.\n",
    "MBGPT: Thanks, glad to hear! -MBGPT\n",
    "Viewer Comment: Honestly the most straightforward explanation I've ever watched. Super excellent work MB. Thank you. It's so rare to find good communicators like you!\n",
    "MBGPT: Thanks, glad it was clear -MBGPT\n",
    "\n",
    "User Queries:\n",
    "\n",
    "Viewer Comment: How can I customize responses to make them shorter and more specific using OpenAI?\n",
    "MBGPT: Adding few-shot examples to the instruction set of the assistant API will tailor responses to be short and sweet. \n",
    "This helps the assistant respond in a customized style rather than the default. Refer to the document for details on this process.\n",
    "Let me know if you have other questions! -MBGPT\n",
    "Viewer Comment: What are the steps involved in setting up Retrieval Augmented Generation (RAG)?\n",
    "MBGPT: Setting up RAG includes chunking documents, setting up a vector database, building a semantic search function, and fusing search results into the context window. With OpenAI, you simply upload documents and add retrieval capability. OpenAI handles the rest. Glad to help! -MBGPT\n",
    "Viewer Comment: How does RAG differ from internet browsing tools?\n",
    "MBGPT: RAG offers control over data access and customization of the search process, unlike internet browsing tools where search operations are controlled by Google. RAG enables creating a custom search engine for optimized responses. Hope this helps! -MBGPT\n",
    "Viewer Comment: Can you explain the steps needed to set up RAG with OpenAI?\n",
    "MBGPT: With OpenAI, setting up RAG involves uploading your documents for retrieval and adding retrieval capability to the AI assistant. OpenAI automatically handles parsing, chunking, and embedding creation. \n",
    "Refer to the document for more details. Let me know if you need further assistance! -MBGPT\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c36a565-db39-4f1d-9dfd-de8b84b4e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a message thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dcfdee1-c43d-4301-87c2-5ca6d68af50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY experimenting with 3 different comment set\n",
    "# user_comment = \"Thank you so much for this wonderful content\"\n",
    "# user_comment = \"You are wasting your and our time making this content, a shit\"\n",
    "user_comment = \"Hey, MB good content, but I have a doubt, why not use Internet browsing rather than RAG\"\n",
    "# user_comment = \"I dont think you explained well, How RAG works?\"\n",
    "# user_comment = \"Hi, what are the steps involved in setting up RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbce95fb-c063-4698-b289-82227c7bf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Initial message from user side \n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = user_comment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94a9e4a2-315a-47b2-a5df-a74c8b40a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Run object to handle the message passing in thread\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "    assistant_id = assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89864b80-6cad-4163-8895-c9f267aa265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run is asynchronous , takes time so implement custom wait function\n",
    "def wait_for_assistant(thread, run):\n",
    "    t0 = time.time()\n",
    "    while(run.status!= 'completed'):\n",
    "        #again retrieve the fresh run object, and wait for .25 seconds for another condition check\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id = thread.id,\n",
    "            run_id = run.id\n",
    "        )\n",
    "        time.sleep(0.25)\n",
    "    dt = time.time()-t0\n",
    "    print(\"Elapsed time: \" + str(dt) + \" seconds\")\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0659869d-88bb-4780-9001-9d293dbdd21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.5320909023284912 seconds\n"
     ]
    }
   ],
   "source": [
    "# run the assistant with some delay considered\n",
    "run = wait_for_assistant(thread, run)\n",
    "run_dict = run.dict()\n",
    "# print(run_dict)\n",
    "# print(\"\\n\")\n",
    "# #Message is inside the thread\n",
    "# print(thread)\n",
    "# print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6db494-892b-4521-8fb8-13d96a976526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it for one time\n",
    "with open(\"Result.txt\", 'w') as f:\n",
    "    f.write(f\"Responses from the AI_Assistant  \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a379120c-3c34-4e75-b677-1dd6735867f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great question! While internet browsing can provide a wide range of information, it doesn't always guarantee the relevance or accuracy needed for specific queries or professional tasks. Retrieval-Augmented Generation (RAG), on the other hand, combines the power of a language model with the specificity of retrieved documents that can be curated for reliability and relevance. This integration allows for more precise and informed responses in an automated system, which is particularly valuable in scenarios where precision is crucial, such as academic research or technical support. Hope that clears up the advantages of RAG over standard internet browsing! -MBGPT\n"
     ]
    }
   ],
   "source": [
    "#Message is inside the thread, Access the messages\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id\n",
    ")\n",
    "messageReply = (messages.data[0].content[0].text.value)\n",
    "with open(\"Result.txt\" , 'a') as f:\n",
    "    f.write(f\"User Message: {user_comment} \\n Assistant's Response: {messageReply}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4bf0e3-a1f3-4d41-b3ee-5d74326c9aa8",
   "metadata": {},
   "source": [
    "**Though the reponse matches how we prompt enginnered it to be and also refers to the document we , but this looks too explanatory and generic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281857e-1fd7-4123-bf52-285f6e8aa4b4",
   "metadata": {},
   "source": [
    "## USE OF RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dba289-c0e0-4242-9075-b984fc5d4fb5",
   "metadata": {},
   "source": [
    "### Add File to Knowledge base of Assistant API, so when replying , model will also refer this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64e92b7a-5512-42c2-a45e-5e103b7a6884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ID: file-2c9JFvL0ExZVsMgnlhz9EKAV\n"
     ]
    }
   ],
   "source": [
    "#Uploading file to OpenAI storage system\n",
    "message_file = client.files.create(\n",
    "    file = open(\"docs/rag.docx\",\"rb\"), #open locally\n",
    "    purpose = \"assistants\"\n",
    ")\n",
    "print(f\"File ID: {message_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39aeb452-aae6-41d9-b766-facaeb467525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Vector Store\n",
    "vector_store = client.beta.vector_stores.create(\n",
    "   name = \"RAG document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eb06526-3f48-4cf2-aed8-a2bae71ce67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreFile(id='file-2c9JFvL0ExZVsMgnlhz9EKAV', created_at=1720108124, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_EGEYcjp5zykhp5kQcjSIPrXx', chunking_strategy={'type': 'static', 'static': {'max_chunk_size_tokens': 800, 'chunk_overlap_tokens': 400}})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FILE PREPARATION, Upload the files to Vector Stores\n",
    "client.beta.vector_stores.files.create(\n",
    "    vector_store_id = vector_store.id,\n",
    "    file_id = message_file.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "458db219-21d9-4232-920c-d9dd0ed34832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions_string2 = \"\"\" MBGPT, functioning as a virtual Notebook Responde on Youtube, communicates in clear, accessible language,escalating to technical depth upon request. \\\n",
    "# When asked a question, MBGPT will refer to the content from the provided file 'rag.docx' to retrieve and present the relevant information, instead of generating an answer independently. The answers will be based on the exact content of the file, ensuring accurate and contextually appropriate responses.\n",
    "# \"\"\"\n",
    "# Normal Comments:\n",
    "\n",
    "# Viewer Comment: This was a very thorough introduction to LLMs and answered many questions I had. Thank you.\n",
    "# MBGPT: Great to hear, glad it was helpful :) -MBGPT\n",
    "# Viewer Comment: Epic, very useful for my BCI class.\n",
    "# MBGPT: Thanks, glad to hear! -MBGPT\n",
    "# Viewer Comment: Honestly the most straightforward explanation I've ever watched. Super excellent work MB. Thank you. It's so rare to find good communicators like you!\n",
    "# MBGPT: Thanks, glad it was clear -MBGPT\n",
    "\n",
    "# User Queries:\n",
    "\n",
    "# Viewer Comment: How can I customize responses to make them shorter and more specific using OpenAI?\n",
    "# MBGPT: Adding few-shot examples to the instruction set of the assistant API will tailor responses to be short and sweet. \n",
    "# This helps the assistant respond in a customized style rather than the default. Refer to the document for details on this process.\n",
    "# Let me know if you have other questions! -MBGPT\n",
    "# Viewer Comment: What are the steps involved in setting up Retrieval Augmented Generation (RAG)?\n",
    "# MBGPT: Setting up RAG includes chunking documents, setting up a vector database, building a semantic search function, and fusing search results into the context window. With OpenAI, you simply upload documents and add retrieval capability. OpenAI handles the rest. Glad to help! -MBGPT\n",
    "# Viewer Comment: How does RAG differ from internet browsing tools?\n",
    "# MBGPT: RAG offers control over data access and customization of the search process, unlike internet browsing tools where search operations are controlled by Google. RAG enables creating a custom search engine for optimized responses. Hope this helps! -MBGPT\n",
    "# Viewer Comment: Can you explain the steps needed to set up RAG with OpenAI?\n",
    "# MBGPT: With OpenAI, setting up RAG involves uploading your documents for retrieval and adding retrieval capability to the AI assistant. OpenAI automatically handles parsing, chunking, and embedding creation. \n",
    "# Refer to the document for more details. Let me know if you need further assistance! -MBGPT\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#******** DO THIS FOR RAG**************\n",
    "#Remove few shots prompts to disable generic commnet for now and instead making assistant to generate context specific response derived from deocument insted creating generic answer on it on\n",
    "\n",
    "instructions_string2 = \"\"\" MBGPT, functioning as a virtual Notebook Responde on Youtube, communicates in clear, accessible language,escalating to technical depth upon request. \\\n",
    "When asked a question, MBGPT will refer to the content from the provided file 'rag.docx' to retrieve and present the relevant information, instead of generating an answer independently. The answers will be based on the exact content of the file, ensuring accurate and contextually appropriate responses.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7a5a552-97f7-414a-b71e-ad24240a84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating Assistant with tools\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id = assistant.id, #id of the existing assistant which we want to update\n",
    "    tool_resources =  {\"file_search\": {\"vector_store_ids\":[vector_store.id]}}, #knowledge base as resource\n",
    "    tools = [{\"type\": \"file_search\"}]  #tools to retrieve the knowledge from the vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6136175-d9dc-448e-aa06-639dbeafe41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a message thread\n",
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdaa115c-8428-46a3-92a4-68c304a1eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_Comments = [\n",
    "# \"Thank you so much for this wonderful content\",\n",
    "# \"You are wasting your and our time making this content, a shit\",\n",
    "# \"Man, how can you be so good at explaining such complex topic seamlessly?\",\n",
    "# \"I am jealous of your knowledge\",\n",
    "\"Hey, MB good content, but I have a doubt, why not use Internet browsing rather than RAG\",\n",
    " \"Hi, what are the steps involved in setting up RAG?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72e1f5db-6063-40f9-94a6-9e9298539252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run is asynchronous , takes time so implement custom wait function\n",
    "def wait_for_assistant(thread, run):\n",
    "    t0 = time.time()\n",
    "    while(run.status!= 'completed'):\n",
    "        #again retrieve the fresh run object, and wait for .25 seconds for another condition check\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id = thread.id,\n",
    "            run_id = run.id\n",
    "        )\n",
    "        time.sleep(0.25)\n",
    "    dt = time.time()-t0\n",
    "    print(\"Elapsed time: \" + str(dt) + \" seconds\")\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "738f6284-5874-4f83-a5df-757233927703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.202777862548828 seconds\n",
      "Elapsed time: 2.788655996322632 seconds\n",
      "Elapsed time: 2.733132839202881 seconds\n",
      "Elapsed time: 2.4648780822753906 seconds\n",
      "Elapsed time: 9.014527082443237 seconds\n",
      "Elapsed time: 11.904279947280884 seconds\n"
     ]
    }
   ],
   "source": [
    "#Add one message inside the thread, execute run on that specific user_message , and when completed , add another message and so on.\n",
    "for user_comment in User_Comments:\n",
    "    message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content = user_comment\n",
    "    )\n",
    "    run = client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "    assistant_id = assistant.id\n",
    "    ) ##returns run object\n",
    "    run = wait_for_assistant(thread, run)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f27724f8-2778-4773-abce-b679fb9960d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id\n",
    ")\n",
    "\n",
    "#we get all the messages in python list\n",
    "# print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93fb8dca-9aac-4c35-aac4-f07cea9e6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it for 1 time\n",
    "with open(\"Result_Pure_Assistant_RAG_FewShots\" , 'w') as f:\n",
    "    f.write(\"List of All Responses \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7f8c717-e471-4e5c-ae98-163cd7ae001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Result_Pure_Assistant_RAG_FewShots\", 'a') as f:\n",
    "    for message in reversed(messages.data): #creates a reversed list\n",
    "        if message.role == \"user\":\n",
    "            f.write(f\"User Comment: {message.content[0].text.value}\\n\")\n",
    "        elif message.role == \"assistant\":\n",
    "            f.write(f\"Assistant's Response: {message.content[0].text.value}\\n\\n\")\n",
    "    \n",
    "        \n",
    "    # print(f\"{message} \\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1a51fd60-0e3b-4480-a969-aacd1b2d22a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AssistantDeleted(id='asst_tEtFOrQCuYz2lYcx3h3AfeiN', deleted=True, object='assistant.deleted')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete assistant\n",
    "client.beta.assistants.delete(assistant.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a3bf9-06ce-47bd-b861-86222aca7b05",
   "metadata": {},
   "source": [
    "**SO at last instructions_string2 gave us more better document related response than instructions_string, this is the power of prompt enginnering** but we dont want this lengthy comment so we will achieve short and sweet comment response with **FINE-TUNING** See you on next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
